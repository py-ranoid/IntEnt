{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intent Recognition with Sequential Models and Word2Vec\n",
    "The goal of this notebook will be to classify intents of sentences. <br>For the purpose of demonstration, we will be using the ATIS (Airline travel information system) dataset. \n",
    "This can be accomplished with the following steps:\n",
    "- Reading the dataset (from iob files) and Understanding the labels\n",
    "- Encoding the intent labels\n",
    "- Loading the word2vec model and embedding the words.\n",
    "- Creating our sequential model (Bi-RNN) with PyTorch\n",
    "- Testing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the dataset and Understanding labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences : 4978\n",
      "Number of unique intents : 22\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from utils import fetch_data, read_method\n",
    "\n",
    "sents,labels,intents = fetch_data('data2/atis.train.w-intent.iob')\n",
    "\n",
    "def display(n):\n",
    "    sense = []\n",
    "    print (\"INTENT : \",intents[n])\n",
    "    for i in range(len(sents[n])):\n",
    "    #     sense.append({\"word_index\":word_indices[0][i],\"word\":words2idx[word_indices[0][i]],\"entity_index\":name_entities[0][i],\"entity\":tables2idx[name_entities[0][i]],\"label_index\":labels[0][i],\"label\":labels2idx[labels[0][i]]})\n",
    "        sense.append({\"word\":sents[n][i],\"label\":labels[n][i]})\n",
    "    return pd.DataFrame(sense)\n",
    "\n",
    "print (\"Number of sentences :\",len(sents))\n",
    "print (\"Number of unique intents :\",len(set(intents)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INTENT :  atis_ground_fare\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>O</td>\n",
       "      <td>what</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>O</td>\n",
       "      <td>price</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>O</td>\n",
       "      <td>is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>O</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B-transport_type</td>\n",
       "      <td>limousine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>O</td>\n",
       "      <td>service</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>O</td>\n",
       "      <td>in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>B-city_name</td>\n",
       "      <td>boston</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              label       word\n",
       "0                 O       what\n",
       "1                 O      price\n",
       "2                 O         is\n",
       "3                 O          a\n",
       "4  B-transport_type  limousine\n",
       "5                 O    service\n",
       "6                 O         in\n",
       "7       B-city_name     boston"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sents - List of sentences where each sentence is a list of words\n",
    "# intents - List of labelled intents\n",
    "display(random.randint(0,len(sents)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ~~Loading~~ Training the word2vec model and embedding the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training word2vec model\n",
    "from gensim.models import word2vec\n",
    "\n",
    "file_names = read_method.keys()\n",
    "data_sets = []\n",
    "for f in file_names:\n",
    "    data_sets.append(fetch_data(f))\n",
    "\n",
    "all_sents = []    \n",
    "all_intents = []\n",
    "for temp_sents,_,temp_intents in data_sets:\n",
    "    all_sents += list([list(x)+['EOS'] for x in temp_sents])\n",
    "    all_intents += list(temp_intents)\n",
    "    \n",
    "w2v_model = word2vec.Word2Vec(all_sents,min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from gensim.models import KeyedVectors\n",
    "# MODEL_PATH = '/home/b/Downloads/GoogleNews-vectors-negative300.bin.gz'\n",
    "# w2v_model = KeyedVectors.load_word2vec_format(MODEL_PATH, binary=True,limit=2500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_sentence(sent):\n",
    "    return [w2v_model.wv[word] for word in list(sent)+['EOS']]\n",
    "\n",
    "enc_sents = []\n",
    "exceptions = []\n",
    "for s in sents:\n",
    "    try:\n",
    "        enc_sents.append(embed_sentence(s))\n",
    "    except KeyError:\n",
    "        exceptions.append(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding the intent labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Intents</th>\n",
       "      <th>Encoded Intents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>atis_flight</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>atis_flight</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>atis_flight_time</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>atis_airfare</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>atis_airfare</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Intents  Encoded Intents\n",
       "0       atis_flight               14\n",
       "1       atis_flight               14\n",
       "2  atis_flight_time               19\n",
       "3      atis_airfare                3\n",
       "4      atis_airfare                3"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "intent_encoder = preprocessing.LabelEncoder()\n",
    "intent_encoder.fit(all_intents)\n",
    "\n",
    "enc_intents = intent_encoder.transform(intents)\n",
    "\n",
    "target = torch.LongTensor(enc_intents).unsqueeze_(-1)\n",
    "\n",
    "pd.DataFrame({\"Intents\":intents[:5],\"Encoded Intents\":enc_intents[:5]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating our sequential model (Bi-RNN) with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.in2hid_fwd = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.in2hid_bck = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        \n",
    "        self.hid2out = nn.Linear(hidden_size*2, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "                    \n",
    "        hidden_fwd = self.initHidden()        \n",
    "        \n",
    "        for word in sentence:        \n",
    "            temp_comb = (torch.from_numpy(word).view(1,-1), hidden_fwd)\n",
    "            combined_fwd = torch.cat(temp_comb, 1)\n",
    "            hidden_fwd = self.in2hid_fwd(combined_fwd)\n",
    "        \n",
    "        hidden_bck = self.initHidden()        \n",
    "        \n",
    "        for word in sentence[::-1]:\n",
    "            temp_comb = (torch.from_numpy(word).view(1,-1), hidden_fwd)\n",
    "            combined_bck = torch.cat(temp_comb, 1)\n",
    "            hidden_bck = self.in2hid_bck(combined_bck)\n",
    "            \n",
    "        combined_full = torch.cat((hidden_fwd, hidden_bck), 1)\n",
    "        \n",
    "        output = self.hid2out(combined_full)\n",
    "        output = self.softmax(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNN(input_size=w2v_model.vector_size,\n",
    "          hidden_size=50, \n",
    "          output_size=len(intent_encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-3.2375, -3.4819, -3.1829, -3.3999, -3.5318, -3.3369, -3.2144,\n",
       "          -3.1258, -3.2889, -3.1266, -3.2126, -3.1669, -3.3000, -3.4782,\n",
       "          -3.2032, -3.3715, -3.1530, -3.3392, -3.0339, -3.2316, -3.1438,\n",
       "          -3.2253, -3.4502, -3.1610, -3.2873, -3.2226]]), 3.2031524181365967)"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate = 0.005 \n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "def train(sentence, intent):    \n",
    "    rnn.zero_grad()\n",
    "\n",
    "    output = rnn(sentence)\n",
    "    \n",
    "    loss = criterion(output, intent.long())\n",
    "    loss.backward()\n",
    "\n",
    "    for p in rnn.parameters():\n",
    "        p.data.add_(-learning_rate, p.grad.data)\n",
    "\n",
    "    return output, loss.item()\n",
    "\n",
    "train(enc_sents[0],target[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00s since start | (Epoch : 1, 50%) Loss : 3.1132\n",
      "1.39s since start | (Epoch : 1, 50%) Loss : 0.0712\n",
      "2.54s since start | (Epoch : 1, 50%) Loss : 0.2806\n",
      "3.63s since start | (Epoch : 1, 50%) Loss : 1.2845\n",
      "4.71s since start | (Epoch : 1, 50%) Loss : 0.0203\n",
      "5.78s since start | (Epoch : 2, 100%) Loss : 0.0418\n",
      "6.87s since start | (Epoch : 2, 100%) Loss : 0.0649\n",
      "7.97s since start | (Epoch : 2, 100%) Loss : 8.6046\n",
      "9.12s since start | (Epoch : 2, 100%) Loss : 0.0580\n",
      "10.22s since start | (Epoch : 2, 100%) Loss : 0.0440\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "n_iters = 2\n",
    "print_every = 1000\n",
    "all_losses = []\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for iter in range(1, n_iters + 1):\n",
    "    for x in range(len(enc_sents)):\n",
    "        output, loss = train(enc_sents[x],target[x])\n",
    "#         print (output,loss)\n",
    "        if math.isnan(x):\n",
    "            print (\"NAN loss\")\n",
    "            break\n",
    "\n",
    "        total_loss += loss\n",
    "\n",
    "        if x % print_every == 0:\n",
    "            print('%.2fs since start | (Epoch : %d, %d%%) Loss : %.4f' % (time.time()-start, iter, iter / n_iters * 100, loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 78.61142217245241\n"
     ]
    }
   ],
   "source": [
    "def test_one(sent,val,allow=3):\n",
    "    pred = rnn(sent).topk(allow)[1].tolist()[0]\n",
    "    return val in pred\n",
    "\n",
    "def test():\n",
    "    sents_test,_,intents_test = fetch_data('data2/atis.test.w-intent.iob')\n",
    "    enc_intents_test = intent_encoder.transform(intents_test)\n",
    "    target_test = torch.LongTensor(enc_intents_test).unsqueeze_(-1)\n",
    "    \n",
    "    num_correct = 0.0\n",
    "    for sent,targ in zip(sents_test,target_test):\n",
    "        sent = embed_sentence(sent)    \n",
    "        if test_one(sent,targ,allow=1):\n",
    "            num_correct+=1\n",
    "            \n",
    "    print (\"Accuracy :\",num_correct/len(sents_test)*100)\n",
    "\n",
    "test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
